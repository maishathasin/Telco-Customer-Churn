---
title: > 
  `\vspace{-1.5cm}`{=latex} STAT 441 Project Final Report
author: "Maisha Thasin, Joseph Wang (Group 25, undergraduate)"
date: "Winter 2023"
output: 
  pdf_document:
    md_extensions: +hard_line_breaks
geometry: margin=1.5cm
urlcolor: blue
bibliography: report.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary
- Do this last

# Introduction
- Talk about the issue (churn) and its applications
- Describe the dataset
- Introduce the research question
- EDA

## Dataset
The Telco Customer Churn datset, provided by IBM, contains information about a fictional telecommunications (telco) company which provided home phone and internet services to 7043 customers. The dataset provides demographic and business-related metrics for each customer, as well as identifying whether the customer switched providers (customer churn).

The dataset can be downloaded from the following link: <https://accelerator.ca.analytics.ibm.com/bi/?perspective=authoring&pathRef=.public_folders%2FIBM%2BAccelerator%2BCatalog%2FContent%2FDAT00067>

The dataset contains **33** variables for **7043** observations, but not all variables are fit to be predictive features. We removed columns relating to unique IDs, geographical information, and dashboarding aggregation, as well as "duplicate" columns (those that are identical to another column except for formatting), and columns related to the response (such as the churn reason and predicted lifetime value to the company).

We are then left with **19** features: Gender, Senior Citizen, Partner, Dependents, Tenure Months (integer/numeric), Phone Service, Multiple Lines, Internet Service, Online Security, Online Backup, Device Protection Plan, Tech Support, Streaming TV, Streaming Movies, Contract, Paperless Billing, Payment method, Monthly Charge (float/numeric), and Total Charges (float/numeric). Each feature is categorical except those specified otherwise. The descriptions of the features can be found at <https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2019/07/11/telco-customer-churn-1113>.

Finally, the response variable is Churn Value, binary on whether the customer left the company this quarter.

# Methods
## Preprocessing
In our initial investigation, we identified which columns of the dataset we were going to use as features, as well as the label. We then determined the type of each feature, with all but three being categorical (including binary features), and the remainder being of unbounded numeric type. We then removed incomplete rows, leaving 7032 observations.

The preprocessing steps, including the train/test split, were done in a unified manner so that each model would be trained and tested on the exact same data, so that accurate comparisons can be made from the results. For use in a supervised classification task, we also introduced some additional preprocessing steps (not all of which are used by every model).

The first optional step is Synthetic Minority Over-sampling Technique (SMOTE) [@Chawla_2002]. SMOTE is an over-sampling method intended to reduce the bias towards the majority class in classification models by artificially creating new datapoints in the minority class. The authors note that "the combination of SMOTE and under-sampling also performs better... than the methods that could directly handle the skewed class distribution". The minority class in our dataset represents approximately 24% of the entire population, so there may be a benefit for some models to apply SMOTE and under-sampling. This is implemented in our preprocessing using the imblearn package, of which we are using the SMOTENC (for datasets including categorical features) and RandomUnderSampler methods. For the rest of the report, SMOTE will refer to the combination of synthetic over-sampling and random under-sampling the training set.

The second optional step is the one-hot encoding step. Given our large number of categorical features, many of which are non-binary and not ordinal, some(?) classifiers require this step in order to interpret the data correctly. Each label in a categorical variate is turned into a new binary variate representing the existence of that label. There are two versions of the one-hot encoding step; the first has all labels given their own variate, increasing the number of features from 19 to 46; the second has one label per category set as a baseline and not given its own variate (i.e. dropped), increasing the number of features from 19 to 30.

The third optional step is feature scaling. For our three numeric features, all three are non-negative and unbounded, and some models perform better(?) with bounded numerical values. Hence, the numeric features are rescaled to be between 0 and 1 inclusive.

All three optional preprocessing steps are applied independently to the training and testing sets to ensure no leakage occurs. Additionally, all preprocessing steps and models are seeded whenever possible to ensure reproducibility and deterministic behavior.

## Logistic Regression
Logistic regression is a widely used algorithm in machine learning, especially for binary classification problems like churn cases. We modeled 2 logistic regression models using different feature engineering techniques one with raw data and one with Undersampling techniques.

 To find the best hyper parameters for each model, we used a grid search function in sklearn called GridSearchCV and standardized the data by splitting it into five folds for each model. We used two types of penalty, LASSO and Ridge regression, which correspond to l1 and l2 penalties, respectively for it's hyperparameters. We also tuned the regularization parameter C, where higher values of C indicate that the training data is more representative, while smaller C values imply that the training data is less representative.
 
To further improve our models, we utilized the Recursive Feature Elimination (RFE) method, which eliminates features that reduce the cross-validation score recursively. By selecting only the most relevant features, RFE can help improve the model's accuracy, reduce overfitting, and enhance interpretability. 
Each model was tested for accuracy, F1 score, and ROC score. To test for overfitting, we repeated the tuned model over 1000 samples and found its average accuracy score. 

## KNN
In our implementation of the K-Nearest Neighbors (KNN) algorithm, we employed the GridSearchCV method again  to determine the optimal hyperparameters . Specifically, we tuned the number of neighbors, considering various distance metrics such as Minkowski, Euclidean, Manhattan, and Cosine. The Minkowski distance metric is a generalized version of Manhattan and Euclidean distances in a vector norm space.

The Minkowski distance between two points `p` and `q` in `n`-dimensional space is defined as:

$$ \text{dist}_\text{M}(p,q)=\left(\sum_{i=1}^{n} |p_i - q_i|^r \right)^{1/r}$$
where:
- `p` and `q` are two points in n-dimensional space
- `n` is the number of dimensions
- `r` is a parameter that determines the order of the Minkowski distance

In the special case where `r=1`, we get the Manhattan distance:
$$ \text{dist}_\text{Manhattan}(p,q)=\sum_{i=1}^{n} |p_i - q_i|$$
In the special case where `r=2`, we get the Euclidean distance:

$$ \text{dist}_\text{Euclidean}(p,q)=\sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}$$

We additionally used to 2 different weight hyperparamters, ‘uniform’ and ‘distance’ The uniform weight parameter assigns equal weights to all data points, whereas the distance weight parameter assigns higher weights to closer neighbors and lower weights to farther neighbors

We employed three different models that does the above hyper parameter optimization. The first model used raw data, the second model used Under sampling + MinMax scaling, and the third model used SMOTE + Robust Scaler a scaling that is robust to outliers (ref). We assessed the accuracy of each of these models  tested their accuracy and F1 score, as well as their ROC score.
To test for overfitting, we repeated the tuned model over 1000  bootstrapped samples. 


## Decision Tree
The Decision Tree model is implemented with sklearn's DecisionTreeClassifier, and using one-hot encoded data. First, we tune the hyperparameter $\alpha$, the tuning parameter used in cost-complexity pruning.

Using k-fold cross-validation with k=5, the trees were fully grown using recursive binary splitting, and then repeatedly pruned back to obtain a sequence of subtrees. Then, the average test error (on the unused fold) is computed as a function of $\alpha$, and the $\alpha$ which maximizes the average test error is chosen.

Then, a Decision Tree is fit on the entire training set, then pruned back using $\alpha$ to return the final decision tree model.

Two versions of this model are fitted; one using the training set, and another using the training set returned from SMOTE.

## Random Forest
The Random Forest model is implemented with sklearn's RandomForestClassifier, and using one-hot encoded data. First, we tune the hyperparameter $d$, the maximum depth of each tree in the forest.

Using k-fold cross-validation with k=5, we used sklearn's GridSearchCV to find the best value of $d$ which maximized expected test accuracy. Each forest consists of 100 trees, and we elected not to tune the number of random features at each split, selecting the square root of the total number of features.

Then, a Random Forest is fit on the entire training set, again using 100 trees, the square root of the number of features, and using the best $d$ chosen in the previous step. This is then returned as the final random forest model.

Two versions of this model are fitted; one using the training set, and another using the training set returned from SMOTE.

## Neural Network (Multi-Layer Perceptron)
The Neural Network model is implemented with sklearn's MLPClassifier (Multi-Layer Perceptron Classifier), and using one-hot encoded and scaled data.

First, using k-fold cross-validation with k=5, we used sklearn's GridSearchCV to tune the hyperparameters for activation function, solver (optimization algorithm), and hidden layer sizes, evaluated on expected. On the first pass, a wide range of hidden layer sizes is provided to GridSearchCV.

GridSearchCV returns the optimal parameters from the parameters provided to maximize the expected accuracy. Since there are a huge number of possible hidden layer sizes (widths and depths), the first pass only selects a few which are far apart. Then, the activation function and solver method are fixed, and we repeat tuning the hidden layer sizes hyperparameter multiple times, each time narrowing the search based on the optimal parameters returned from the last iteration. A detailed breakdown of one such repeated hyperparameter tuning step is included as a multi-line comment in the code, which can be found in the appendix.

Finally, a Multi-Layer Perceptron is fit on the entire training set with the chosen hyperparameters to be the final model.

Two versions of this model are fitted; one with the number of hidden layers fixed to 1 (i.e. a single layer perceptron), and another without this restriction.

# Results
- Compare performance
- Key graphs go here
- Discuss performance, significant predictors

| Model | Accuracy | F1 label = 0 | F1 label = 1 | F1 | Hyperparameters |
|-------|----------|----------|----|----|----------------|
| Logistic regression (raw) | 0.841 | 0.89 | 0.71 | ? | ['penalty':['l1','l2'], 
              'C':[1, 10, 100, 1000]] |
| Logistic regression (SMOTE + Scale) | 0.87 | 0.82 | 120 | 25 | K=7, Distance=Euclidean |
| Model 3 | 0.83 | 0.76 | 95 | 18 | K=3, Distance=Manhattan |
| Model 4 | 0.89 | 0.84 | 130 | 23 | K=9, Distance=Cosine |
| Model 5 | 0.82 | 0.75 | 90 | 17 | K=5, Distance=Minkowski |
| Model 6 | 0.88 | 0.83 | 125 | 21 | K=7, Distance=Euclidean |
| Model 7 | 0.84 | 0.77 | 97 | 19 | K=3, Distance=Manhattan |
| Model 8 | 0.90 | 0.85 | 135 | 22 | K=9, Distance=Cosine |
| Model 9 | 0.81 | 0.74 | 88 | 16 | K=5, Distance=Minkowski |
| Model 10 | 0.89 | 0.83 | 123 | 24 | K=7, Distance=Euclidean |
| Model 11 | 0.85 | 0.78 | 98 | 18 | K=3, Distance=Manhattan |
| Model 12 | 0.91 | 0.86 | 138 | 21 | K=9, Distance=Cosine |

# Discussion
- Discuss existing Kaggle models on this dataset
- Strengths and limitations of models
- Future research

# Appendix
## References

<div id="refs"></div>

## Code
### Dataset
General purpose Dataset class to unify the loading, preprocessing, and evaluations of the different models.
```{python, eval=FALSE, echo=TRUE, python.reticulate=FALSE, code=readLines("telco.py")}
```

### Decision Tree
```{python, eval=FALSE, echo=TRUE, python.reticulate=FALSE, code=readLines("dtree.py")}
```

### Random Forest
```{python, eval=FALSE, echo=TRUE, python.reticulate=FALSE, code=readLines("rforest.py")}
```

### Neural Network (Multi-layer Perceptron)
```{python, eval=FALSE, echo=TRUE, python.reticulate=FALSE, code=readLines("nnet.py")}
```
